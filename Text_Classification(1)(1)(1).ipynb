{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AkEoCKaHdpj"
      },
      "source": [
        "\n",
        "In this notebook, we will try the process of implementing RNN with Keras in order to classify text sentences.\n",
        "\n",
        "I.   **Firstly**, we'll import useful packages.\n",
        "\n",
        "II.   **Then**, we'll load the data and create a word embedding matrix using Glove.\n",
        "\n",
        "III.  **We'll try a simple RNN model** and then we will evaluate its performances.\n",
        "\n",
        "IV. Finally, we'll use techniques to increase our model's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xY_w9I1cZni"
      },
      "source": [
        "**Task 1:** Setting Fre GPU in this Google Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4iAL0E0ciDS"
      },
      "source": [
        "## Mounting Google Drive locally\n",
        "**Task 2:** Mount the Google Driver into the Google Colab Driver.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I8iz8Rp8H5pG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649a4d78-6512-4900-e4a7-54bdc45a2ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ],
      "source": [
        "## TYPE YOUR CODE for task 2 here:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeAakuO9cD5s"
      },
      "source": [
        "# I. Let import all useful packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx22h2phGoJu",
        "outputId": "af5bfe8a-edf6-418c-8847-dcd495227660"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "TWgEP6KSHmV_"
      },
      "outputs": [],
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import tensorflow.keras\n",
        "import datetime\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, Callback\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.metrics import TruePositives, FalsePositives, FalseNegatives\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from sklearn.metrics import confusion_matrix as CM\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plot\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvFTfBIscRwC"
      },
      "source": [
        "**Task 3**: Copy the dataset from Google Drive into Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MzzQsanZIZfg"
      },
      "outputs": [],
      "source": [
        "## TYPE YOUR CODE for task 3 here:\n",
        "!cp gdrive/MyDrive/dataset/asm2/train.csv .\n",
        "!cp gdrive/MyDrive/dataset/asm2/glove.6B.50d.txt ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_GxFMl7dFJ-"
      },
      "source": [
        "# II. Load the data.\n",
        "\n",
        "## About dataset.\n",
        "An invalid question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is invalid:\n",
        "\n",
        "* Has a non-neutral tone.\n",
        "* Is disparaging or inflammatory.\n",
        "* Isn't grounded in reality.\n",
        "* Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n",
        "\n",
        "The data includes the question that was asked, and whether it was identified as invalid (target = 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HhWwT-gpuN"
      },
      "source": [
        "**Task 4**: Load the dataset.\n",
        "* Load the data from CSV file.\n",
        "* Remove all the rows with NA values.\n",
        "* Split the data into 3 set: Training set, validation set and test set (0.9/0.05/0.05, random_seed = 9) with a same ratio of data number beween each class.\n",
        "* Print out these dataset's description.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j9HMbZrqK1Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d578f5-bff0-4748-e521-36f007fa628e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1.175509e+06\n",
            "mean     6.187022e-02\n",
            "std      2.409198e-01\n",
            "min      0.000000e+00\n",
            "25%      0.000000e+00\n",
            "50%      0.000000e+00\n",
            "75%      0.000000e+00\n",
            "max      1.000000e+00\n",
            "Name: label, dtype: float64\n",
            "count    65306.000000\n",
            "mean         0.061863\n",
            "std          0.240908\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          1.000000\n",
            "Name: label, dtype: float64\n",
            "count    65307.000000\n",
            "mean         0.061877\n",
            "std          0.240934\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          0.000000\n",
            "max          1.000000\n",
            "Name: label, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(data_link):\n",
        "    '''\n",
        "    input: data link.\n",
        "    output:\n",
        "        train_set, validation_set and test_set(0.95/0.05/0.05) without NA values.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 4 here:\n",
        "    df = pd.read_csv(data_link).dropna().iloc[:, 1:]  # drop id\n",
        "    df.columns = ['text', 'label']\n",
        "\n",
        "    # Split 0.9 for train\n",
        "    train, validation_n_test = train_test_split(df, train_size=0.9, random_state=9, stratify=df['label'])\n",
        "    # Split half for validation and half test (0.05 each)\n",
        "    validation, test = train_test_split(validation_n_test, test_size=0.5, random_state=9, stratify=validation_n_test['label'])\n",
        "\n",
        "    return train, validation, test\n",
        "\n",
        "train_set, validation_set, test_set = load_data('train.csv')\n",
        "print(train_set['label'].describe())\n",
        "print(validation_set['label'].describe())\n",
        "print(test_set['label'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIcOnRkbqofC"
      },
      "source": [
        "# Encoding text data.\n",
        "Let declare some fundamental parameters first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0F3_zcCjHwzm"
      },
      "outputs": [],
      "source": [
        "embed_size = 50 # how big is each word vector\n",
        "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 50 # max number of words in a question to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT8m71iixjxt"
      },
      "source": [
        "**Task 5:** Encode the dataset using Tokenizer and one-hot encoding vector.\n",
        "* Encode the text (question_text column) by turning each question text into a list of word indexes using [Tokenizer](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do) with **max_features** and all the text sentences from the training and the validation set. \n",
        "* Turn each list of word indexes into an equal length - **max_len** (with truncation or padding as needed) using [pad_sequences](https://keras.io/preprocessing/sequence/).\n",
        "* Encode the label (label column) using [to_categorical](https://keras.io/utils/) function on Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q1MZKNs4xmfP"
      },
      "outputs": [],
      "source": [
        "def encoding_textdata(train_set, validation_set, test_set, max_features, max_len):\n",
        "    '''\n",
        "    Input:\n",
        "    - Train/validation/test dataset.\n",
        "    - max_features, max_len.\n",
        "    Output:\n",
        "    - X train/validation/test, y train/validation/test.\n",
        "    - Tokenizer.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 5 here:\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    tokenizer.fit_on_texts(pd.concat([train_set['text'], validation_set['text']]))\n",
        "    X_tr = tokenizer.texts_to_sequences(train_set['text'])\n",
        "    X_tr = pad_sequences(X_tr, maxlen=max_len)    # use default padding and truncation i.e. 'pre'\n",
        "    y_tr = to_categorical(train_set['label'])\n",
        "    X_va = tokenizer.texts_to_sequences(validation_set['text'])\n",
        "    X_va = pad_sequences(X_va, maxlen=max_len)\n",
        "    y_va = to_categorical(validation_set['label'])\n",
        "    X_te = tokenizer.texts_to_sequences(test_set['text'])\n",
        "    X_te = pad_sequences(X_te, maxlen=max_len)\n",
        "    y_te = to_categorical(test_set['label'])\n",
        "\n",
        "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), tokenizer\n",
        "\n",
        "(X_tr, y_tr), (X_va, y_va), (X_te, y_te), tokenizer = encoding_textdata(train_set, validation_set, test_set, max_features, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check result\n",
        "print(X_tr.shape)\n",
        "X_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4K0ffWHiYy",
        "outputId": "a1820452-41b7-4001-fd71-c38c5110e536"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1175509, 50)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  3767,   391,   258],\n",
              "       [    0,     0,     0, ..., 18261,    46,  1864],\n",
              "       [    0,     0,     0, ...,     4, 16538,   562],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,  6150,    27,   286],\n",
              "       [    0,     0,     0, ...,    23,   951,  2184],\n",
              "       [    0,     0,     0, ...,     1,   933, 11291]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_tr.shape)\n",
        "y_tr"
      ],
      "metadata": {
        "id": "yXnv73F6Cz26",
        "outputId": "e4679ead-1b3d-40a2-9429-2e8d2fd8f48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1175509, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kpG-p30WUcc"
      },
      "source": [
        "**Task 6**: Create word embedding matrix.\n",
        "* Firstly, write a function to [load the GloVe dictionary.](https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db)\n",
        "* Then, create a word embedding matrix using GloVe dictionary with these parameters:\n",
        "    - Word embedding matrix shape: (Number of word, embed_size).\n",
        "    - Embed size: 50.\n",
        "    - Number of words: The minimum of (max_features, len(word_index)), while word_index is the dictionary of word which contains in tokenizer.\n",
        "    - If a word occurs in GloVe dictionary, we should take its initialization value as in GloVe dictionary. Otherwise, take a normal random value with mean and std as mean and std of GloVe dictionary value.\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "47s8-SncWT3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb3a3d6-c601-4f0f-94ca-c1937b1e0dcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
              "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
              "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
              "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
              "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
              "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
              "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
              "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
              "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
              "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def get_coefs(word,*arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "def get_GloVe_dict(GloVe_link):\n",
        "    '''\n",
        "    input: GloVe link.\n",
        "    output: GloVe dictionary.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 6 here:\n",
        "    GloVe_dict = {}\n",
        "    with open(GloVe_link) as f:\n",
        "        for line in f:\n",
        "            arr = line.split(' ')\n",
        "            key, val = get_coefs(arr[0], *arr[1:])\n",
        "            GloVe_dict[key] = val\n",
        "    return GloVe_dict\n",
        "\n",
        "    \n",
        "GloVe_link = 'glove.6B.50d.txt'\n",
        "GloVe_dict = get_GloVe_dict(GloVe_link)\n",
        "GloVe_dict['the']  # Check result with word 'the'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DXRyFSLtr4_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b545981-edf4-461d-f1a6-480f1b7f1b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 50)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
              "        -0.11514   , -0.78580999],\n",
              "       [ 0.45322999,  0.059811  , -0.10577   , ...,  0.53240001,\n",
              "        -0.25103   ,  0.62546003],\n",
              "       ...,\n",
              "       [-0.10721   , -1.36220002,  1.33169997, ...,  0.64011002,\n",
              "         0.063936  , -1.74650002],\n",
              "       [ 0.18455   , -0.68822998, -0.20072   , ..., -0.95519   ,\n",
              "        -0.030273  , -0.31542   ],\n",
              "       [-0.34103999,  0.17725   , -0.54510999, ..., -1.19029999,\n",
              "        -0.20367999, -0.169     ]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "def create_embedding_matrix(GloVe_dict, tokenizer, max_features):\n",
        "    '''\n",
        "    input: GloVe dictionaray, tokenizer from training and validation dataset, number of max features.\n",
        "    output: Word embedding matrix.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 6 here:\n",
        "    word_num = min(max_features, len(tokenizer.index_word))\n",
        "\n",
        "    # Calculate mean and std\n",
        "    df = pd.DataFrame.from_dict(GloVe_dict, orient='index')\n",
        "    embed_size = df.shape[1]\n",
        "    coefs = df.stack()\n",
        "    mean = coefs.mean()\n",
        "    std = coefs.std()\n",
        "\n",
        "    # List of words in tokenizer (sort by index i.e. from max count)\n",
        "    words = [tokenizer.index_word[idx] for idx in range(1, word_num)]\n",
        "\n",
        "    # Create embedding matrix with world indices from tokenizer and coefs from GloVe_dict\n",
        "    embedding_matrix = np.zeros((word_num, embed_size))\n",
        "    # For unknow word\n",
        "    embedding_matrix[0, :] = 0\n",
        "    for idx in range(1, word_num):\n",
        "        embedding_matrix[idx, :] = GloVe_dict.get(words[idx-1], np.random.normal(mean, std, embed_size))\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(GloVe_dict, tokenizer, max_features)\n",
        "print(embedding_matrix.shape)\n",
        "embedding_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWybjdQkqWrg"
      },
      "source": [
        "III. Modelling\n",
        "There are some steps we need to finish:\n",
        "Build the model.\n",
        "\n",
        "Compile the model.\n",
        "\n",
        "Train / fit the data to the model.\n",
        "\n",
        "Evaluate the model on the testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6AMfqQkqcET"
      },
      "source": [
        "## Build the model\n",
        "**Task 7:** We can build an easy model composed of different layers such as:\n",
        "* [Embedding](https://keras.io/layers/embeddings/) layer with max_features, embed_size and embedding_matrix.\n",
        "* [Bidirectional LSTM layer](https://keras.io/examples/nlp/bidirectional_lstm_imdb/?fbclid=IwAR3fEd6aWyeIDEhZSspjtCRiP0c0Jnz5-XdnUHQYwX8Tp8k9Ni4I8Q5tP9o) with number of hidden state = 50, dropout_rate = 0.1 and recurrent_dropout_rate = 0.1.\n",
        "* GlobalMaxPool1D.\n",
        "* Dense with number of unit = 50, activation = 'relu'.\n",
        "* Dropout with rate = 0.1.\n",
        "* Final dense with number of unit = number of class, activation = 'sigmoid'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "J7_eizWaqi_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff69636a-e9c7-45a3-cdd9-3d4b7a9bab43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "def create_model(max_len, max_features, embed_size):\n",
        "    '''\n",
        "    input: max_len, max_features, embed_size\n",
        "    output: model.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 7 here:\n",
        "    i = Input(shape=(max_len,))\n",
        "    x = Embedding(max_features, embed_size, mask_zero=True,\n",
        "                  embeddings_initializer=tensorflow.keras.initializers.constant(embedding_matrix),\n",
        "                  trainable=False)(i)\n",
        "    x = Bidirectional(LSTM(units=25, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    o = Dense(2, activation='sigmoid')(x)\n",
        "    model = Model(i, o)\n",
        "    return model\n",
        "\n",
        "model = create_model(max_len, max_features, embed_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWcBKhzMux9Z"
      },
      "source": [
        "**Task 8:** Compile the model and setup the callback. Then print out the model summary.\n",
        "* [Compile](https://keras.io/models/model/#compile) the model with Adam Optimizaer, lr = 1e-2, suitable loss for binary classification problem and [\"F1-score\"](https://github.com/tensorflow/addons/issues/825) as metric.\n",
        "* Print out the model summary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class F1Callback(Callback):\n",
        "    def __init__(self, X_va, y_va):\n",
        "        super().__init__()\n",
        "        self.X_va = X_va\n",
        "        self.y_va = y_va.argmax(1)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs['val_f1_score'] = f1_score(self.y_va, self.model.predict(self.X_va).argmax(1), average='weighted')\n",
        "\n",
        "class PrintCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(\"Epoch {}:\".format(epoch), end='')\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\" loss: {:.3}, acc: {:.3}, val_acc: {:.3}, val_f1: {:.3}\".format(logs['loss'],\n",
        "                                                                               logs['accuracy'],\n",
        "                                                                               logs['val_accuracy'],\n",
        "                                                                               logs['val_f1_score']))"
      ],
      "metadata": {
        "id": "su92IIQRaCc8"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "P9l8EbG0ur1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6b3151-a00d-4059-adde-38af43ef2e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_14 (InputLayer)       [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding_13 (Embedding)    (None, 50, 50)            1000000   \n",
            "                                                                 \n",
            " bidirectional_13 (Bidirecti  (None, 50, 50)           15200     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_max_pooling1d_13 (Gl  (None, 50)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 50)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 2)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,017,852\n",
            "Trainable params: 17,852\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def optimize(model):\n",
        "    '''\n",
        "    Input: \n",
        "        Model.\n",
        "    Return: \n",
        "        Complied model.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 8 here:\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-2),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = optimize(model)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BlenccGzLVr"
      },
      "source": [
        "**Task 9**: Setup callback.\n",
        "* Create the [tensorboard callback](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks) to save the logs.\n",
        "* Create the [checkpoint callback](https://machinelearningmastery.com/check-point-deep-learning-models-keras/) to save the checkpoint with the best accuracy after each epoch.\n",
        "* Create the [ReduceLROnPlateau](https://keras.io/callbacks/#reducelronplateau) callback with factor=0.3, patience=1 and \"Validation F1-score\" monitor.\n",
        "* Create the [early stopping callback](https://keras.io/callbacks/#earlystopping) with patience=7, mode = 'max' and \"Validation F1-score\" monitor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "I6x6dteutin0"
      },
      "outputs": [],
      "source": [
        "def callback_model(checkpoint_name, logs_name):\n",
        "    '''\n",
        "    Input: \n",
        "        Best checkpoint name, logs name.\n",
        "    Return: \n",
        "        Callback list, which contains tensorboard callback and checkpoint callback.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 9 here:\n",
        "    f1_cb = F1Callback(X_va, y_va)\n",
        "    pr_cb = PrintCallback()\n",
        "    ts_cb = TensorBoard(logs_name)\n",
        "    cp_cb = ModelCheckpoint(checkpoint_name,\n",
        "                            monitor='val_f1_score',\n",
        "                            save_best_only=True,\n",
        "                            save_weights_only=True,\n",
        "                            mode='max',\n",
        "                            save_freq='epoch')\n",
        "    lr_cb = ReduceLROnPlateau(monitor='val_f1_score', factor=0.3, patience=1, mode='max')\n",
        "    es_cb = EarlyStopping(monitor='val_f1_score', patience=7, mode='max', min_delta=0.002)\n",
        "\n",
        "    return [f1_cb, pr_cb, ts_cb, cp_cb, lr_cb, es_cb]\n",
        "\n",
        "checkpoint_name = 'weights.best.hdf5'\n",
        "logs_name = 'training_logs'\n",
        "callbacks_list = callback_model(checkpoint_name, logs_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nDCsHAC2HwW"
      },
      "source": [
        "**Task 10:** Train the model.\n",
        "\n",
        "* Train the model with 20 epochs with batch_size = 4096.\n",
        "* Return the model with best-checkpoint weights.\n",
        "\n",
        "*Hint*: Fit the model first, then reload the model (load_model function) with best-checkpoint weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xttwiHh4u0ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6964411-e5c4-4027-f322-c956711686c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:Epoch 1/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9501 loss: 0.129, acc: 0.95, val_acc: 0.953, val_f1: 0.951\n",
            "287/287 [==============================] - 430s 1s/step - loss: 0.1289 - accuracy: 0.9501 - val_loss: 0.1193 - val_accuracy: 0.9533 - val_f1_score: 0.9508 - lr: 0.0100\n",
            "Epoch 1:Epoch 2/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9530 loss: 0.121, acc: 0.953, val_acc: 0.955, val_f1: 0.952\n",
            "287/287 [==============================] - 473s 2s/step - loss: 0.1206 - accuracy: 0.9530 - val_loss: 0.1151 - val_accuracy: 0.9552 - val_f1_score: 0.9521 - lr: 0.0100\n",
            "Epoch 2:Epoch 3/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9541 loss: 0.118, acc: 0.954, val_acc: 0.955, val_f1: 0.952\n",
            "287/287 [==============================] - 432s 2s/step - loss: 0.1176 - accuracy: 0.9541 - val_loss: 0.1137 - val_accuracy: 0.9553 - val_f1_score: 0.9524 - lr: 0.0100\n",
            "Epoch 3:Epoch 4/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9547 loss: 0.116, acc: 0.955, val_acc: 0.956, val_f1: 0.952\n",
            "287/287 [==============================] - 409s 1s/step - loss: 0.1160 - accuracy: 0.9547 - val_loss: 0.1128 - val_accuracy: 0.9558 - val_f1_score: 0.9522 - lr: 0.0100\n",
            "Epoch 4:Epoch 5/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9557 loss: 0.113, acc: 0.956, val_acc: 0.956, val_f1: 0.954\n",
            "287/287 [==============================] - 466s 2s/step - loss: 0.1134 - accuracy: 0.9557 - val_loss: 0.1112 - val_accuracy: 0.9559 - val_f1_score: 0.9540 - lr: 0.0030\n",
            "Epoch 5:Epoch 6/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9557 loss: 0.113, acc: 0.956, val_acc: 0.956, val_f1: 0.955\n",
            "287/287 [==============================] - 413s 1s/step - loss: 0.1126 - accuracy: 0.9557 - val_loss: 0.1111 - val_accuracy: 0.9560 - val_f1_score: 0.9548 - lr: 0.0030\n",
            "Epoch 6:Epoch 7/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9562 loss: 0.112, acc: 0.956, val_acc: 0.956, val_f1: 0.954\n",
            "287/287 [==============================] - 471s 2s/step - loss: 0.1121 - accuracy: 0.9562 - val_loss: 0.1106 - val_accuracy: 0.9561 - val_f1_score: 0.9542 - lr: 0.0030\n",
            "Epoch 7:Epoch 8/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9566 loss: 0.111, acc: 0.957, val_acc: 0.956, val_f1: 0.954\n",
            "287/287 [==============================] - 434s 2s/step - loss: 0.1114 - accuracy: 0.9566 - val_loss: 0.1102 - val_accuracy: 0.9563 - val_f1_score: 0.9543 - lr: 9.0000e-04\n",
            "Epoch 8:Epoch 9/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9565 loss: 0.111, acc: 0.956, val_acc: 0.956, val_f1: 0.954\n",
            "287/287 [==============================] - 475s 2s/step - loss: 0.1111 - accuracy: 0.9565 - val_loss: 0.1102 - val_accuracy: 0.9562 - val_f1_score: 0.9544 - lr: 2.7000e-04\n",
            "Epoch 9:Epoch 10/20\n",
            "287/287 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9565 loss: 0.111, acc: 0.957, val_acc: 0.956, val_f1: 0.955\n",
            "287/287 [==============================] - 436s 2s/step - loss: 0.1108 - accuracy: 0.9565 - val_loss: 0.1103 - val_accuracy: 0.9562 - val_f1_score: 0.9545 - lr: 8.1000e-05\n",
            "Epoch 10:Epoch 11/20\n",
            "251/287 [=========================>....] - ETA: 30s - loss: 0.1106 - accuracy: 0.9568"
          ]
        }
      ],
      "source": [
        "def train_model(model, callbacks_list):\n",
        "    '''\n",
        "    Input: \n",
        "        Model and callback list,\n",
        "    Return: \n",
        "        Model with best-checkpoint weights.\n",
        "    '''\n",
        "    ## TYPE YOUR CODE for task 10 here:\n",
        "    model.fit(X_tr, y_tr,\n",
        "              epochs=20,\n",
        "              batch_size=4096,\n",
        "              validation_data=(X_va, y_va),\n",
        "              callbacks=callbacks_list)\n",
        "    model.load_weights(callbacks_list[3].filepath)\n",
        "    return model\n",
        "\n",
        "model = train_model(model, callbacks_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp weights.best.hdf5 gdrive/MyDrive/dataset/asm2/"
      ],
      "metadata": {
        "id": "mBPFbK7TaO1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r training_logs gdrive/MyDrive/dataset/asm2/"
      ],
      "metadata": {
        "id": "RdiUHD3vajbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsG02Ao07Mc-"
      },
      "source": [
        "**Task 11:** Show the tensorboard in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpBk-EKZ2Ut7"
      },
      "outputs": [],
      "source": [
        "## TYPE YOUR CODE for task 11 here:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir training_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1ed1CY8Rxh"
      },
      "source": [
        "**Task 12:** Prediction on test set.\n",
        "\n",
        "* Complete the get_prediction_classes function.\n",
        "* Print out the precision, recall and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHTjBLZYvx26"
      },
      "outputs": [],
      "source": [
        "def get_prediction_classes(model, X, y):\n",
        "    ## TYPE YOUR CODE for task 13 here:\n",
        "    '''\n",
        "    Input: \n",
        "        Model and prediction dataset.\n",
        "    Return: \n",
        "        Prediction list and groundtrurth list with predicted classes.\n",
        "    '''\n",
        "    groundtruths = y.argmax(1)\n",
        "    prediction = model.predict(X).argmax(1)\n",
        "    return predictions, groundtruths\n",
        "\n",
        "test_predictions, test_groundtruths = get_prediction_classes(model,  X_te, y_te)\n",
        "print(precision_score(test_predictions, test_groundtruths), average='weighted')\n",
        "print(recall_score(test_predictions, test_groundtruths), average='weighted')\n",
        "print(f1_score(test_predictions, test_groundtruths), average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJwQahjp8hZs"
      },
      "source": [
        "**Task 13:** Perform the predicted result on test set using confusion matrix. Remember to show the class name in the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKAmOGvv2Be"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(predictions, groundtruth, class_names):\n",
        "    ## TYPE YOUR CODE for task 13 here:\n",
        "    p = ConfusionMarixDisplay(confusion_matrix(groundtruth, predictions))\n",
        "    plot.show()\n",
        "class_names = ['valid', 'invalid']\n",
        "plot_confusion_matrix(test_predictions, test_groundtruths, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGmUAFDi87Z6"
      },
      "source": [
        "**Task 14**: Model finetuning - fine tune the model using some of these approachs:\n",
        "* Increase max epochs, change batch size.\n",
        "* Replace LSTM by GRU units and check if it changes anything.\n",
        "* Add another layer of LSTM/GRU, see if things improve.\n",
        "* Play around with Dense layers (add/# units/etc).\n",
        "* Find preprocessing rules you could add to improve the quality of the data.\n",
        "* Find another GloVe dictionary.\n",
        "Requirement: The F1 score should increase by 2-3%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELLyczmROE8J"
      },
      "outputs": [],
      "source": [
        "## TYPE YOUR CODE for task 14 here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8fS_x7XJFmQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Classification(1)(1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}